---

vault_pass: "{{ vault_vault_pass }}"
all_authorized_key_users: "{{ vault_all_authorized_key_users }}"
openstack_privatekeys: "{{ vault_openstack_privatekeys }}"
ceph_galaxy_scratch_key: "{{ vault_ceph_galaxy_scratch_key }}"
slurm_scale_tailscale_authkey: "{{ vault_slurm_scale_tailscale_authkey }}"

clouds_yaml: "{{ vault_clouds_yaml }}"
cloud_id: js2
controller_name: jetstream2

# These would be the values for the TACC JS2 cloud
#cloud_id: js2-tacc
#os_public_network: provider

jetstream_local_controller_address: "{{ controller_name }}.js2local"
jetstream_local_cvmfs_stratum1_address: "cvmfs1-iu0.js2local"
jetstream_local_cvmfs_proxy_address: "{{ controller_name }}.js2local"

# Used by the image generation playbook, instances names match the image name
usegalaxy_node_image_name: "{{ inventory_hostname }}"

# Controls whose keys are added to the admin user on cloud instances in the openstack role (names match those in
# sshkeys provided by the sshservers group vars)
os_admin_users: "{{ galaxy_admin_users }}"

# This turns out to be unnecessary
#openstack_userdata: |
#  #cloud-config
#  manage_resolv_conf: true
#  resolv_conf:
#    searchdomains:
#      - jetstreamlocal
#      - galaxyproject.org
#    domain: galaxyproject.org

ceph_mounts:
  - name: galaxy-scratch
    path: /jetstream2/scratch
    key: "{{ ceph_galaxy_scratch_key }}"
    src: 149.165.158.38:6789,149.165.158.22:6789,149.165.158.54:6789,149.165.158.70:6789,149.165.158.86:6789:/volumes/_nogroup/778b0312-d418-4290-a0b0-57b41bc72baf/92efe188-bf18-43d5-a07a-836b2fb5afe3
    opts: noatime,_netdev
    # kernel client keeps wrecking MDS's and FUSE client seems to still be decently performant so, gonna run that for now
    fstype: fuse.ceph
    #fstype: ceph

os_cloud_id: "{{ cloud_id }}"
os_clouds_yaml: "{{ clouds_yaml }}"

os_name: "{{ inventory_hostname_short }}"

# FIXME: this doesn't work when launching the controller itself, I used my own key
os_key_name: admin-kp

os_nics:
  - net-name: "usegalaxy"

galaxy_cvmfs_repos_enabled: yes

all_groups:
  - name: G-803372
    gid: 803372
  - name: idc
    gid: 808

all_users:
  - name: g2test
    group: G-803372
    groups: docker
    comment: 'Galaxy Test Server'
    uid: 819456
    home: /home/g2test
    shell: /bin/bash
  - name: g2main
    group: G-803372
    groups: docker
    comment: 'Galaxy Main Server'
    uid: 819800
    home: /home/g2main
    shell: /bin/bash
  - name: idc
    group: idc
    comment: 'Intergalactic Data Commission'
    uid: 808
    home: /home/idc
    shell: /bin/sh

# Prefer the JS2 stratum 1 via the local network
galaxy_cvmfs_server_urls:
  - domain: galaxyproject.org
    use_geoapi: true
    urls:
      - "http://cvmfs1-iu0.js2local/cvmfs/@fqrn@"
      - "http://cvmfs1-iu0.galaxyproject.org/cvmfs/@fqrn@"
      - "http://cvmfs1-tacc0.galaxyproject.org/cvmfs/@fqrn@"
      - "http://cvmfs1-psu0.galaxyproject.org/cvmfs/@fqrn@"

cvmfs_http_proxies:
  - "http://jetstream2.js2local:3128"
  - "DIRECT"

slurm_user:
  uid: 40302
  gid: 40302
  group: slurm
  comment: 'Slurm Workload Manager'
  home: /var/spool/slurm
  shell: /bin/bash

slurm_cluster_name: "{{ cloud_id | replace('_', '-') }}"
slurm_controller_name: "{{ controller_name }}"

slurm_munge_key: files/slurm/munge.key

# what mounts/dirs/users the prolog script will check
slurm_prolog_cvmfs_repos:
  - main.galaxyproject.org
  - data.galaxyproject.org
  - singularity.galaxyproject.org

slurm_prolog_dirs:
  - /jetstream2/scratch

slurm_prolog_users:
  - g2main

slurm_config:
  SlurmctldHost: "{{ slurm_controller_name }}"
  ClusterName: "{{ slurm_cluster_name }}"

  UnkillableStepTimeout: 300

  PluginDir: /usr/lib64/slurm

  ReturnToService: 1

  JobCompLoc: /var/log/slurm/slurm.job.log
  JobCompType: jobcomp/filetxt
  SchedulerType: sched/backfill
  SchedulerParameters: nohold_on_prolog_fail

  Prolog: /etc/slurm/prolog.*
  PrologEpilogTimeout: 90

  SelectType: select/cons_res
  SelectTypeParameters: CR_CPU_Memory
  SwitchType: switch/none

  SlurmctldTimeout: 300
  SlurmdSpoolDir: /var/spool/slurm/d/slurmd.spool
  SlurmdTimeout: 300
  StateSaveLocation: /var/spool/slurm/ctld/slurm.state

  # TODO: disabled until TACC slurm is upgraded
  #AccountingStorageType: accounting_storage/slurmdbd
  #AccountingStorageHost: galaxy08.tacc.utexas.edu

  JobAcctGatherType: jobacct_gather/cgroup
  JobAcctGatherFrequency: task=15
  ProctrackType: proctrack/cgroup
  TaskPlugin: task/cgroup

  SlurmctldLogFile: /var/log/slurm/slurmctld.log
  SlurmctldDebug: 4
  SlurmdLogFile: /var/log/slurm/slurmd.log
  SlurmdDebug: 4

  # Cloud configuration
  PrivateData: cloud
  ResumeProgram: "/opt/slurmscale/sbin/slurm_resume"
  SuspendProgram: "/opt/slurmscale/sbin/slurm_suspend"
  ResumeFailProgram: "/opt/slurmscale/sbin/slurm_suspend"
  ResumeRate: 8        # number of nodes per minute that can be created; 0 means no limit
  ResumeTimeout: 900   # max time in seconds between ResumeProgram running and when the node is ready for use
  SuspendRate: 8       # number of nodes per minute that can be suspended/destroyed
  SuspendTime: 600     # time in seconds before an idle node is suspended
  SuspendTimeout: 300  # time between running SuspendProgram and the node being completely down

slurm_nodes:
  - name: "js2-tiny[0-7]"
    State: CLOUD
    Weight: 10
    CPUs: 1
    RealMemory: 2840  # 2984, then 2919, then 2896 reported by slurmd
  - name: "js2-small[0-7]"
    State: CLOUD
    Weight: 20
    CPUs: 2
    RealMemory: 5900  # 5937 reported by slurmd
  - name: "js2-quad[0-7]"
    State: CLOUD
    Weight: 30
    CPUs: 4
    RealMemory: 14800  # 14819 reported by slurmd
  - name: "js2-medium[0-23]"
    State: CLOUD
    Weight: 40
    CPUs: 8
    RealMemory: 29900  # 29938 reported by slurmd
  - name: "js2-large[0-23]"
    State: CLOUD
    Weight: 50
    CPUs: 16
    RealMemory: 60100  # 60176 reported by slurmd
  - name: "js2-xl[0-15]"
    State: CLOUD
    Weight: 60
    CPUs: 32
    RealMemory: 125000  # 125798 reported by slurmd
  #- name: "js2-2xl[0-1]"
  #  State: CLOUD
  #  Weight: 70
  #  CPUs: 64
  #  RealMemory: 250000  # 251678 reported by slurmd
  - name: "js2-mem-large[0-17]"
    State: CLOUD
    Weight: 80
    CPUs: 64
    RealMemory: 502000  # 503677 reported by slurmd
  - name: "js2-mem-xl[0-15]"
    State: CLOUD
    Weight: 90
    CPUs: 128
    RealMemory: 1004000  # 1007851 reported by slurmd
  - name: "js2-gpu-small[0-3]"
    State: CLOUD
    CPUs: 4
    RealMemory: 14800  # 14819 reported by slurmd
  - name: "js2-gpu-medium[0-3]"
    State: CLOUD
    CPUs: 8
    RealMemory: 29900  # ????? reported by slurmd
  - name: "js2-gpu-large[0-1]"
    State: CLOUD
    CPUs: 16
    RealMemory: 60100  # ????? reported by slurmd
  - name: "js2-gpu-xl[0-1]"
    State: CLOUD
    CPUs: 32
    RealMemory: 125000  # ????? reported by slurmd
  - name: "js2-gxit-small[0-1]"
    State: CLOUD
    CPUs: 2
    RealMemory: 5900  # same as js2-small

slurm_partitions:
  - name: vgp
    Nodes: "js2-tiny[0-1],js2-small[0-1],js2-quad[0-1],js2-medium[0-1],js2-large[0-1],js2-xl[0-1],js2-mem-large[0-5],js2-mem-xl[0-5]"
    DefaultTime: "06:00:00"
    MaxTime: "96:00:00"
    MaxNodes: 1
    State: UP
  - name: priority
    Nodes: "js2-tiny[2-3],js2-small[2-3],js2-quad[2-3],js2-medium[2-3],js2-large[2-7],js2-xl[2-7]"
    DefaultTime: "06:00:00"
    MaxTime: "96:00:00"
    MaxNodes: 1
    State: UP
  - name: tpv
    Default: YES
    Nodes: "js2-tiny[4-7],js2-small[4-7],js2-quad[4-7],js2-medium[4-23],js2-large[8-23],js2-xl[8-15],js2-mem-large[6-15],js2-mem-xl[6-15]"
    DefaultTime: "06:00:00"
    MaxTime: "96:00:00"
    MaxNodes: 1
    State: UP
  - name: resize-shm
    Nodes: "js2-mem-large[16-17]"
    DefaultTime: "06:00:00"
    MaxTime: "96:00:00"
    MaxNodes: 1
    State: UP
    OverSubscribe: EXCLUSIVE
  - name: gpu-small
    DefaultTime: "24:20:00"
    MaxTime: "96:20:00"
    Nodes: "js2-gpu-small[0-3]"
    MaxNodes: 1
    State: UP
    OverSubscribe: EXCLUSIVE
  - name: gpu-medium
    DefaultTime: "24:20:00"
    MaxTime: "96:20:00"
    Nodes: "js2-gpu-medium[0-3]"
    MaxNodes: 1
    State: UP
    OverSubscribe: EXCLUSIVE
  - name: gpu-large
    DefaultTime: "24:20:00"
    MaxTime: "96:20:00"
    Nodes: "js2-gpu-large[0-1]"
    MaxNodes: 1
    State: UP
    OverSubscribe: EXCLUSIVE
  - name: gpu-xl
    DefaultTime: "24:20:00"
    MaxTime: "96:20:00"
    Nodes: "js2-gpu-xl[0-1]"
    MaxNodes: 1
    State: UP
    OverSubscribe: EXCLUSIVE
  - name: gxit
    Nodes: "js2-gxit-small[0-1]"
    DefaultTime: "12:20:00"
    MaxTime: "96:20:00"
    MaxNodes: 1
    State: UP

slurm_cgroup_config:
  CgroupAutomount: yes
  ConstrainCores: yes
  ConstrainRAMSpace: yes
  ConstrainSwapSpace: yes
