---

slurm_munge_key: files/slurm/munge.key

slurm_config:
  ControlMachine: "{{ slurm_controller_name }}"
  ControlAddr: "{{ slurm_controller_ip }}"
  ClusterName: "{{ slurm_cluster_name }}"

  UnkillableStepTimeout: 300

  PluginDir: /usr/lib64/slurm

  ReturnToService: 1

  JobCompLoc: /var/log/slurm/slurm.job.log
  JobCompType: jobcomp/filetxt
  SchedulerType: sched/backfill
  #SchedulerParameters: nohold_on_prolog_fail

  Prolog: /jetstream2/scratch/slurm/prolog.bash
  Epilog: /jetstream2/scratch/slurm/epilog.bash
  PrologEpilogTimeout: 90

  SelectType: select/cons_res
  SelectTypeParameters: CR_CPU_Memory
  SwitchType: switch/none

  SlurmctldTimeout: 300
  SlurmdSpoolDir: /var/spool/slurm/d/slurmd.spool
  SlurmdTimeout: 300
  StateSaveLocation: /var/spool/slurm/ctld/slurm.state

  AccountingStorageType: accounting_storage/slurmdbd
  AccountingStorageHost: galaxy08.tacc.utexas.edu

  JobAcctGatherType: jobacct_gather/cgroup
  JobAcctGatherFrequency: task=15
  ProctrackType: proctrack/cgroup
  TaskPlugin: task/cgroup

  SlurmctldLogFile: /var/log/slurm/slurmctld.log
  SlurmctldDebug: 4
  SlurmdLogFile: /var/log/slurm/slurmd.log
  SlurmdDebug: 4

  # Cloud configuration
  PrivateData: cloud
  ResumeProgram: "/opt/slurmscale/sbin/slurm_resume"
  SuspendProgram: "/opt/slurmscale/sbin/slurm_suspend"
  ResumeFailProgram: "/opt/slurmscale/sbin/slurm_suspend"
  ResumeRate: 0        # number of nodes per minute that can be created; 0 means no limit
  ResumeTimeout: 900   # max time in seconds between ResumeProgram running and when the node is ready for use
  SuspendRate: 0       # number of nodes per minute that can be suspended/destroyed
  SuspendTime: 600     # time in seconds before an idle node is suspended
  SuspendTimeout: 60   # time between running SuspendProgram and the node being completely down

slurm_nodes:
  - name: "{{ slurm_cluster_name }}-tiny[0-7]"
    State: CLOUD
    Weight: 10
    CPUs: 1
    RealMemory: 2840  # 2984, then 2919, then 2896 reported by slurmd
  - name: "{{ slurm_cluster_name }}-small[0-7]"
    State: CLOUD
    Weight: 20
    CPUs: 2
    RealMemory: 5900  # 5937 reported by slurmd
  - name: "{{ slurm_cluster_name }}-quad[0-7]"
    State: CLOUD
    Weight: 30
    CPUs: 4
    RealMemory: 14800  # 14819 reported by slurmd
  # TODO: [32-47] are not in a partition and should be dropped once all jobs have drained from them
  - name: "{{ slurm_cluster_name }}-medium[0-47]"
    State: CLOUD
    Weight: 40
    CPUs: 8
    RealMemory: 29900  # 29938 reported by slurmd
  - name: "{{ slurm_cluster_name }}-large[0-19]"
    State: CLOUD
    Weight: 50
    CPUs: 16
    RealMemory: 60100  # 60176 reported by slurmd
  - name: "{{ slurm_cluster_name }}-xl[0-19]"
    State: CLOUD
    Weight: 60
    CPUs: 32
    RealMemory: 125000  # 125798 reported by slurmd
  - name: "{{ slurm_cluster_name }}-2xl[0-1]"
    State: CLOUD
    Weight: 70
    CPUs: 64
    RealMemory: 250000  # 251678 reported by slurmd
  - name: "{{ slurm_cluster_name }}-mem-large[0-3]"
    State: CLOUD
    Weight: 80
    CPUs: 64
    RealMemory: 502000  # 503677 reported by slurmd
  - name: "{{ slurm_cluster_name }}-mem-xl[0-1]"
    State: CLOUD
    Weight: 90
    CPUs: 128
    RealMemory: 1004000  # 1007851 reported by slurmd
  - name: "{{ slurm_cluster_name }}-gpu-small[0-3]"
    State: CLOUD
    CPUs: 4
    RealMemory: 14800  # 14819 reported by slurmd
  - name: "{{ slurm_cluster_name }}-gxit-small[0-1]"
    State: CLOUD
    CPUs: 2
    RealMemory: 5900  # same as js2-small

slurm_partitions:
  - name: vgp
    Nodes: "{{ slurm_cluster_name }}-tiny[0-1],{{ slurm_cluster_name }}-small[0-1],{{ slurm_cluster_name }}-quad[0-1],{{ slurm_cluster_name }}-medium[0-1],{{ slurm_cluster_name }}-large[0-1],{{ slurm_cluster_name }}-xl[0-1],{{ slurm_cluster_name }}-2xl[0-1],{{ slurm_cluster_name }}-mem-large[0-1],{{ slurm_cluster_name }}-mem-xl[0-1]"
    DefaultTime: "06:00:00"
    MaxTime: "96:00:00"
    MaxNodes: 1
    State: UP
  - name: tpv
    Default: YES
    Nodes: "{{ slurm_cluster_name }}-tiny[2-7],{{ slurm_cluster_name }}-small[2-7],{{ slurm_cluster_name }}-quad[2-7],{{ slurm_cluster_name }}-medium[2-31],{{ slurm_cluster_name }}-large[2-19],{{ slurm_cluster_name }}-xl[2-19]"
    DefaultTime: "06:00:00"
    MaxTime: "96:00:00"
    MaxNodes: 1
    State: UP
  - name: resize-shm
    Nodes: "{{ slurm_cluster_name }}-mem-large[2-3]"
    DefaultTime: "06:00:00"
    MaxTime: "96:00:00"
    MaxNodes: 1
    State: UP
    OverSubscribe: EXCLUSIVE
  - name: gpu-small
    DefaultTime: "24:20:00"
    MaxTime: "96:20:00"
    Nodes: "{{ slurm_cluster_name }}-gpu-small[0-3]"
    MaxNodes: 1
    State: UP
    OverSubscribe: EXCLUSIVE
  - name: gxit
    Nodes: "{{ slurm_cluster_name }}-gxit-small[0-1]"
    DefaultTime: "12:20:00"
    MaxTime: "96:20:00"
    MaxNodes: 1
    State: UP
  #- name: reserved
  #  Nodes: "{{ slurm_cluster_name }}-large[23-26]"
  #  DefaultTime: "48:20:00"
  #  MaxTime: "96:20:00"
  #  MaxNodes: 1
  #  State: UP
  #  OverSubscribe: EXCLUSIVE

slurm_cgroup_config:
  CgroupAutomount: yes
  ConstrainCores: yes
  ConstrainRAMSpace: yes
  ConstrainSwapSpace: yes
