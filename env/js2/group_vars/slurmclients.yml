---

slurm_munge_key: files/slurm/munge.key

slurm_config:
  ControlMachine: "{{ slurm_controller_name }}"
  ControlAddr: "{{ slurm_controller_ip }}"
  ClusterName: "{{ slurm_cluster_name }}"

  PluginDir: /usr/lib64/slurm

  ReturnToService: 1

  JobCompLoc: /var/log/slurm/slurm.job.log
  JobCompType: jobcomp/filetxt
  SchedulerType: sched/backfill

  SelectType: select/cons_res
  SelectTypeParameters: CR_CPU_Memory
  SwitchType: switch/none

  SlurmctldPort: 7002
  SlurmctldTimeout: 300
  SlurmdPort: 7003
  SlurmdSpoolDir: /var/spool/slurm/d/slurmd.spool
  SlurmdTimeout: 300
  StateSaveLocation: /var/spool/slurm/ctld/slurm.state

  DefaultStorageLoc: /var/log/slurm/slurm_accounting
  AccountingStorageType: accounting_storage/slurmdbd
  AccountingStorageHost: galaxy08.tacc.utexas.edu
  AccountingStoragePort: 7031

  JobAcctGatherType: jobacct_gather/cgroup
  JobAcctGatherFrequency: task=15
  ProctrackType: proctrack/cgroup
  TaskPlugin: task/cgroup

  SlurmctldLogFile: /var/log/slurm/slurmctld.log
  SlurmctldDebug: 4
  SlurmdLogFile: /var/log/slurm/slurmd.log
  SlurmdDebug: 4

  # Cloud configuration
  PrivateData: cloud
  ResumeProgram: "/opt/slurmscale/sbin/slurm_resume"
  SuspendProgram: "/opt/slurmscale/sbin/slurm_suspend"
  ResumeFailProgram: "/opt/slurmscale/sbin/slurm_suspend"
  ResumeRate: 0        # number of nodes per minute that can be created; 0 means no limit
  ResumeTimeout: 900   # max time in seconds between ResumeProgram running and when the node is ready for use
  SuspendRate: 0       # number of nodes per minute that can be suspended/destroyed
  SuspendTime: 600     # time in seconds before an idle node is suspended
  SuspendTimeout: 60   # time between running SuspendProgram and the node being completely down

slurm_nodes:
  - name: "{{ slurm_cluster_name }}-small[0-7]"
    State: CLOUD
    CPUs: 2
    RealMemory: 5748
  - name: "{{ slurm_cluster_name }}-quad[0-7]"
    State: CLOUD
    CPUs: 4
    RealMemory: 14819
  - name: "{{ slurm_cluster_name }}-medium[0-31]"
    State: CLOUD
    CPUs: 8
    RealMemory: 29938
  - name: "{{ slurm_cluster_name }}-large[0-7]"
    State: CLOUD
    CPUs: 16
    RealMemory: 60176
  - name: "{{ slurm_cluster_name }}-xl[0-7]"
    State: CLOUD
    CPUs: 32
    RealMemory: 125000  # 125798 reported by slurmd
  - name: "{{ slurm_cluster_name }}-gpu-small[0-2]"
    State: CLOUD
    CPUs: 4
    RealMemory: 14819

slurm_partitions:
  - name: normal
    Nodes: "{{ slurm_cluster_name }}-small[0-7]"
    Default: YES
    DefaultTime: "48:20:00"
    MaxTime: "96:20:00"
    MaxNodes: 1
    State: UP
    OverSubscribe: EXCLUSIVE
  - name: quad
    Nodes: "{{ slurm_cluster_name }}-quad[0-7]"
    DefaultTime: "48:20:00"
    MaxTime: "96:20:00"
    MaxNodes: 1
    State: UP
  - name: multi
    Nodes: "{{ slurm_cluster_name }}-medium[0-31]"
    DefaultTime: "48:20:00"
    MaxTime: "96:20:00"
    MaxNodes: 1
    State: UP
    OverSubscribe: EXCLUSIVE
  - name: large
    Nodes: "{{ slurm_cluster_name }}-large[0-7]"
    DefaultTime: "48:20:00"
    MaxTime: "96:20:00"
    MaxNodes: 1
    State: UP
    OverSubscribe: EXCLUSIVE
  - name: xl
    Nodes: "{{ slurm_cluster_name }}-xl[0-7]"
    DefaultTime: "48:20:00"
    MaxTime: "96:20:00"
    MaxNodes: 1
    State: UP
    OverSubscribe: EXCLUSIVE
  - name: gpu-small
    DefaultTime: "24:20:00"
    MaxTime: "96:20:00"
    Nodes: "{{ slurm_cluster_name }}-gpu-small[0-1]"
    MaxNodes: 1
    State: UP
  #- name: reserved
  #  Nodes: "{{ slurm_cluster_name }}-large[23-26]"
  #  DefaultTime: "48:20:00"
  #  MaxTime: "96:20:00"
  #  MaxNodes: 1
  #  State: UP
  #  OverSubscribe: EXCLUSIVE

slurm_cgroup_config:
  CgroupAutomount: yes
  ConstrainCores: yes
  ConstrainRAMSpace: yes
  ConstrainSwapSpace: yes
