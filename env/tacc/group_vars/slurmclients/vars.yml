---

slurm_user:
  uid: 40302
  gid: 40302
  comment: 'Slurm Workload Manager'
  home: /var/lib/slurm
  shell: /bin/bash

slurm_controller_name: galaxy08
slurm_controller_ip: 129.114.60.125
slurm_cluster_name: roundup
slurm_dbd_server_name: galaxy08
#slurm_dbd_server_ip: 129.114.60.125

# FIXME: old paths?
#slurmd_spool_dir: /var/lib/slurm/slurmd/slurmd.spool
#slurmctld_state_dir: /var/lib/slurm/slurmctld/slurm.state

slurm_munge_key: files/slurm/munge.key

# what mounts/dirs/users the prolog script will check
slurm_prolog_cvmfs_repos:
  - main.galaxyproject.org
  - data.galaxyproject.org
  - singularity.galaxyproject.org

slurm_prolog_dirs:
  - /corral4/main

slurm_prolog_users:
  - g2main

slurm_config:
  ControlMachine: "{{ slurm_controller_name }}"
  ControlAddr: "{{ slurm_controller_ip }}"
  ClusterName: "{{ slurm_cluster_name }}"

  # Should help decrease the frequency of nodes draining with "Kill task failed"
  # https://bugs.schedmd.com/show_bug.cgi?id=3941
  UnkillableStepTimeout: 300

  PluginDir: /usr/lib64/slurm

  ReturnToService: 1
  RebootProgram: /usr/sbin/reboot

  JobCompLoc: /var/log/slurm/slurm.job.log
  JobCompType: jobcomp/filetxt
  SchedulerType: sched/backfill
  SchedulerParameters: nohold_on_prolog_fail

  Prolog: /etc/slurm/prolog.sh
  PrologEpilogTimeout: 90

  SelectType: select/cons_res
  SelectTypeParameters: CR_CPU_Memory
  SwitchType: switch/none

  SlurmctldTimeout: 300
  #SlurmdSpoolDir: /var/spool/slurm/d/slurmd.spool
  SlurmdSpoolDir: /var/lib/slurm/slurmd/slurmd.spool
  SlurmdTimeout: 300
  #StateSaveLocation: /var/spool/slurm/ctld/slurm.state
  StateSaveLocation: /var/lib/slurm/slurmctld/slurm.state

  DefaultStorageLoc: /var/log/slurm/slurm_accounting
  AccountingStorageType: accounting_storage/slurmdbd
  AccountingStorageHost: "{{ slurm_dbd_server_name }}"

  JobAcctGatherType: jobacct_gather/cgroup
  JobAcctGatherFrequency: task=15
  ProctrackType: proctrack/cgroup
  TaskPlugin: task/cgroup

  SlurmctldLogFile: /var/log/slurm/slurmctld.log
  SlurmctldDebug: 4
  SlurmdLogFile: /var/log/slurm/slurmd.log
  SlurmdDebug: 4

# Weight is set so that nodes exclusive to a partition will be preferred
slurm_nodes:
  # permanent nodes
  - name: roundup[49-54]
    RealMemory: 122880
    CPUS: 24
    State: UNKNOWN
    Weight: 0
  - name: roundup[55-60]
    RealMemory: 122880
    CPUS: 24
    State: UNKNOWN
    Weight: 10
  - name: roundup[61-64]
    RealMemory: 122880
    CPUS: 24
    State: UNKNOWN
    Weight: 0

# Within nodes of a common weight, LLN is used; normal jobs are prioritized on shared nodes
slurm_partitions:
  - name: multi
    State: UP
    MaxTime: 8-00:00:00
    MaxNodes: 1
    DefMemPerCPU: 5120
    MaxCPUsPerNode: 20
    Nodes: roundup[49-60]
    LLN: "YES"
    PriorityTier: 0
  - name: normal
    State: UP
    MaxTime: 8-00:00:00
    MaxNodes: 1
    MaxCPUsPerNode: 20
    DefMemPerCPU: 7680
    Nodes: roundup[55-64]
    LLN: "YES"
    Default: "YES"
    PriorityTier: 10
  - name: priority
    State: UP
    MaxTime: 8-00:00:00
    MaxNodes: 1
    MaxCPUsPerNode: 20
    DefMemPerCPU: 7680
    Nodes: roundup[49-64]
    LLN: "YES"
    PriorityTier: 20
