---

slurm_cluster_name: jetstream-tacc

slurm_munge_key: files/slurm/munge.key

slurm_config:
  ControlMachine: "{{ slurm_controller_name }}"
  ControlAddr: "{{ slurm_controller_ip }}"
  ClusterName: "{{ slurm_cluster_name }}"

  PluginDir: /usr/lib64/slurm

  FastSchedule: 1
  ReturnToService: 1

  JobCompLoc: /var/log/slurm/slurm.job.log
  JobCompType: jobcomp/filetxt
  SchedulerType: sched/backfill

  SelectType: select/cons_res
  SelectTypeParameters: CR_CPU_Memory
  SwitchType: switch/none

  SlurmctldPort: 7002
  SlurmctldTimeout: 300
  SlurmdPort: 7003
  SlurmdSpoolDir: /var/lib/slurm/slurmd/slurmd.spool
  SlurmdTimeout: 300
  StateSaveLocation: /var/lib/slurm/slurmctld/slurm.state

  DefaultStorageLoc: /var/log/slurm/slurm_accounting
  AccountingStorageType: accounting_storage/slurmdbd
  AccountingStorageHost: galaxy08.tacc.utexas.edu
  AccountingStoragePort: 7031

  JobAcctGatherType: jobacct_gather/cgroup
  JobAcctGatherFrequency: task=15
  ProctrackType: proctrack/cgroup
  TaskPlugin: task/cgroup

  SlurmctldLogFile: /var/log/slurm/slurmctld.log
  SlurmctldDebug: 4
  SlurmdLogFile: /var/log/slurm/slurmd.log
  SlurmdDebug: 4

  # TODO: redeployed controller, any way to do this dynamically?
  FirstJobId: 1261014

  # Cloud configuration
  PrivateData: cloud
  ResumeProgram: "/opt/slurmscale/sbin/slurm_resume"
  SuspendProgram: "/opt/slurmscale/sbin/slurm_suspend"
  ResumeFailProgram: "/opt/slurmscale/sbin/slurm_suspend"
  ResumeRate: 0        # number of nodes per minute that can be created; 0 means no limit
  ResumeTimeout: 900   # max time in seconds between ResumeProgram running and when the node is ready for use
  SuspendRate: 0       # number of nodes per minute that can be suspended/destroyed
  SuspendTime: 600     # time in seconds before an idle node is suspended
  SuspendTimeout: 60   # time between running SuspendProgram and the node being completely down

slurm_nodes:
  #- name: "{{ slurm_cluster_name }}-small[0-7]"
  #  State: CLOUD
  #  CPUs: 2
  #  RealMemory: 3789
  #- name: "{{ slurm_cluster_name }}-medium[0-15]"
  #  State: CLOUD
  #  CPUs: 6
  #  RealMemory: 15884
  - name: "{{ slurm_cluster_name }}-large[0-15]"
    State: CLOUD
    CPUs: 10
    RealMemory: 29995
  - name: "{{ slurm_cluster_name }}-xlarge[0-15]"
    State: CLOUD
    CPUs: 24
    RealMemory: 60230

slurm_partitions:
  #- name: small
  #  Nodes: "{{ slurm_cluster_name }}-small[0-7]"
  #  Default: YES
  #  DefaultTime: "48:20:00"
  #  MaxTime: "96:20:00"
  #  MaxNodes: 1
  #  State: UP
  #  OverSubscribe: EXCLUSIVE
  #- name: normal
  #  Nodes: "{{ slurm_cluster_name }}-medium[0-15]"
  #  DefaultTime: "48:20:00"
  #  MaxTime: "96:20:00"
  #  MaxNodes: 1
  #  DefMemPerCPU: 7942
  #  State: UP
  - name: multi
    Nodes: "{{ slurm_cluster_name }}-large[0-7]"
    DefaultTime: "48:20:00"
    MaxTime: "96:20:00"
    MaxNodes: 1
    State: UP
    OverSubscribe: EXCLUSIVE
    Default: YES
  - name: xlarge
    Nodes: "{{ slurm_cluster_name }}-xlarge[0-15]"
    DefaultTime: "48:20:00"
    MaxTime: "96:20:00"
    MaxNodes: 1
    State: UP
    OverSubscribe: EXCLUSIVE

slurm_cgroup_config:
  CgroupAutomount: yes
  ConstrainCores: yes
  ConstrainRAMSpace: yes
  ConstrainSwapSpace: yes
