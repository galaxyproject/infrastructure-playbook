---

- name: Create slurm scale directory
  file:
    path: "{{ slurm_scale_root }}"
    state: directory
    owner: "{{ slurm_scale_user }}"

- name: Install dependencies
  yum:
    name:
      - openssl-devel

- name: Create slurm scale virtualenv
  pip:
    name:
      - pip
      - setuptools
    virtualenv: "{{ slurm_scale_root }}"
    virtualenv_command: /usr/bin/python3 -m venv
    state: latest
  become: yes
  become_user: "{{ slurm_scale_user }}"

# using `latest` with these can be disastrous e.g. if a new major release of Ansible is uploaded or a C extension
# dependency (e.g. cffi) is updated without wheels.
- name: Install slurm scale dependencies
  pip:
    name:
      - ansible>2.7.5
      - shade
      - python-openstackclient
      - openstacksdk
    virtualenv: "{{ slurm_scale_root }}"
  become: yes
  become_user: "{{ slurm_scale_user }}"

- name: Create slurm scale ansible directories
  file:
    path: "{{ item }}"
    state: directory
    owner: "{{ slurm_scale_user }}"
  loop:
    - "{{ slurm_scale_root }}/sbin"
    - "{{ slurm_scale_root }}/etc"
    - "/var/log/slurm/elastic"

# FIXME: this is always changed
- name: Copy ansible files
  synchronize:
    archive: no
    recursive: yes
    links: no
    times: yes
    perms: yes
    copy_links: yes
    src: files/
    dest: "{{ slurm_scale_root }}/etc/ansible"
    rsync_path: "sudo -u {{ slurm_scale_user }} rsync"

# Have to do this post-rsync
- name: Create ansible directories for templates and symlinks
  file:
    path: "{{ slurm_scale_root }}/etc/ansible/{{ item.path | dirname }}"
    state: directory
    owner: "{{ slurm_scale_user }}"
  loop:
    - path: group_vars/all/cloud.yml
    - path: files/slurm/munge.key
    - path: inventory/openstack.yml
    #- path: templates/slurm/slurm.conf.j2
    #- path: templates/slurm.conf.j2

- name: Template ansible files
  template:
    src: "{{ item.path }}.j2"
    dest: "{{ slurm_scale_root }}/etc/ansible/{{ item.path }}"
    owner: "{{ slurm_scale_user }}"
  loop:
    - path: group_vars/all/cloud.yml
    - path: group_vars/all/slurm.yml
    - path: inventory/openstack.yml

#- name: Create symlinks
#  file:
#    path: "{{ slurm_scale_root }}/etc/ansible/{{ item.path }}"
#    src: "{{ item.src }}"
#    state: link
#  loop:
#    - path: templates/slurm/slurm.conf.j2
#      src: /etc/slurm/slurm.conf

- name: Install slurm suspend/resume script
  template:
    src: suspend_resume.sh.j2
    dest: "{{ slurm_scale_root }}/sbin/slurm_suspend"
    owner: "{{ slurm_scale_user }}"
    mode: 0755

- name: Link slurm suspend/resume script
  file:
    dest: "{{ slurm_scale_root }}/sbin/slurm_resume"
    src: "{{ slurm_scale_root }}/sbin/slurm_suspend"
    state: hard
    force: yes    # supposed to be fixed in 2.4 but it's not?

- name: Install private data
  copy:
    content: "{{ item.content }}"
    dest: "{{ slurm_scale_root }}/etc/ansible/{{ item.dest }}"
    owner: "{{ slurm_scale_user }}"
    mode: "0400"
  no_log: yes
  loop:
    - content: "{{ slurm_ssh_key }}"
      dest: "slurm-kp.pem"
    - content: "{{ clouds_yaml | to_nice_yaml }}"
      dest: "clouds.yaml"

# this decrypts the key
- name: Install munge key
  copy:
    src: "{{ slurm_munge_key }}"
    dest: "{{ slurm_scale_root }}/etc/ansible/files/slurm/munge.key"
    owner: "{{ slurm_scale_user }}"
    mode: "0400"

# FIXME: this is always changed
# FIXME: does not force upgrade or fail when the version is incorrect but you don't want to add -f or it will always reinstall
# TODO: versions from requirements.yml
- name: Install Ansible modules from Galaxy
  command: "{{ slurm_scale_root }}/bin/ansible-galaxy role install -p {{ slurm_scale_root }}/etc/ansible/roles {{ item }}"
  #args:
  #  creates: "{{ slurm_scale_root }}/etc/ansible/roles/{{ item }}"
  become: yes
  become_user: "{{ slurm_scale_user }}"
  loop:
    - geerlingguy.repo-epel,1.2.3
    - galaxyproject.cvmfs,0.2.14
    - galaxyproject.slurm,0.1.1

# This is for creating the usegalaxy-node image, but it would probably be ideal if we could just use this playbook for
# slurmscale itself rather than constructing a quasi-playbook as we do above
- name: Create Slurm ssh directory
  file:
    path: /var/spool/slurm/.ssh
    state: directory
    owner: slurm
    group: slurm
    mode: 0700

- name: Create Slurm ssh config
  copy:
    content: |
      UserKnownHostsFile /dev/null
      StrictHostKeyChecking no
    dest: /var/spool/slurm/.ssh/config
    owner: slurm
    group: slurm
    mode: 0600

- name: Clone Playbook
  become: yes
  become_user: slurm
  git:
    dest: "{{ slurm_scale_root }}/etc/infrastructure-playbook"
    recursive: true
    force: true
    repo: https://github.com/galaxyproject/infrastructure-playbook.git

- name: Write vault pass
  copy:
    content: "{{ vault_pass }}"
    dest: "{{ slurm_scale_root }}/etc/infrastructure-playbook/.pass"
    owner: slurm
    group: slurm
    mode: 0600
  no_log: yes

- name: Write clouds.yaml
  copy:
    content: "{{ clouds_yaml | to_nice_yaml }}"
    dest: "{{ slurm_scale_root }}/etc/infrastructure-playbook/clouds.yaml"
    owner: slurm
    group: slurm
    mode: 0600
  no_log: yes

- name: Install roles
  become: yes
  become_user: slurm
  command: "{{ slurm_scale_root }}/bin/ansible-galaxy role install -p roles -r requirements.yml"
  args:
    chdir: "{{ slurm_scale_root }}/etc/infrastructure-playbook"

- name: Write image cleanup script
  copy:
    content: |
      #!/bin/bash
      export OS_CLOUD=js2
      cd "{{ slurm_scale_root }}/etc/infrastructure-playbook"
      while read id name; do
          case "$name" in
              usegalaxy-node-*)
                  echo "Deleting ${name} (${id})"
                  openstack image delete "$id" || echo "WARNING: Image ${id} failed to delete, but this is ok if it is still in use"
                  ;;
          esac
      done < <(openstack image list --private --format value --column ID --column Name)
    dest: "{{ slurm_scale_root }}/bin/clean-usegalaxy-node-images"
    owner: slurm
    group: slurm
    mode: 0755

- name: Create image generation cron job
  cron:
    cron_file: ansible_slurmscale
    user: "{{ slurm_scale_user }}"
    name: usegalaxy-node image
    hour: 0
    minute: 0
    weekday: 0
    job: ". {{ slurm_scale_root }}/bin/activate && make -C {{ slurm_scale_root }}/etc/infrastructure-playbook usegalaxy-node-image"

- name: Create image cleanup cron job
  cron:
    cron_file: ansible_slurmscale
    user: "{{ slurm_scale_user }}"
    name: usegalaxy-node image cleanup
    hour: 23
    minute: 0
    job: ". {{ slurm_scale_root }}/bin/activate && {{ slurm_scale_root }}/bin/clean-usegalaxy-node-images"
