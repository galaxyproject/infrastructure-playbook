---

- name: Define groups for resume/spawn nodes
  hosts: galaxynodes
  gather_facts: false
  environment:
     OS_CLOUD: "{{ cloud_id }}"
     OS_IDENTITY_API_VERSION: '3'
  tasks:

    - name: Create existing nodes group
      group_by:
        key: startnodes
      when: openstack is defined

    - name: Create new nodes group
      group_by:
        #key: spawnnodes
        key: startnodes
      when: openstack is not defined


- name: Start existing instance(s)
  hosts: startnodes
  gather_facts: false
  environment:
     OS_CLOUD: "{{ cloud_id }}"
     OS_IDENTITY_API_VERSION: '3'
  tasks:

    - name: Instance start block
      block:

        #- name: Start instance
        #  os_server_action:
        #    cloud: "{{ cloud_id }}"
        #    server: "{{ inventory_hostname }}"
        #    action: "start"
        #  delegate_to: localhost

        - name: Spawn new instance
          os_server:
            cloud: "{{ cloud_id }}"
            name: "{{ inventory_hostname }}"
            image: "{{ image }}"
            flavor: "{{ flavor }}"
            key_name: "{{ key_name }}"
            nics: "{{ nics }}"
            security_groups: "{{ security_groups }}"
            auto_ip: "{{ auto_ip }}"
            meta: group=galaxynodes
            userdata: |
              #cloud-config
              package_upgrade: false
          delegate_to: localhost
          register: spawned_out

        - name: Update inventory with spawned instance IP
          set_fact:
            ansible_host: "{{ spawned_out.server.access_ipv4 or spawned_out.server.addresses[nics[0]['net-name']][0].addr }}"
          delegate_to: localhost

        # WARNING: Slurm's ResumeTimeout must be larger than however long this *entire* playbook might run for, and this
        # task alone will wait up to 600 seconds.
        - name: Wait for instance to become accessible
          wait_for_connection:

        - name: Gather Facts
          gather_facts:
          become: true

        - name: Set CVMFS cache size for instance
          lineinfile:
            path: /etc/cvmfs/default.local
            regexp: '^CVMFS_QUOTA_LIMIT=.*'
            line: 'CVMFS_QUOTA_LIMIT="{{ (0.6 * ((ansible_mounts | selectattr("mount", "==", "/") | first).size_available) / 1024**2) | int }}"'
            mode: "0644"
          become: true

        - name: Connect to tailnet
          command: "/usr/bin/tailscale up --auth-key {{ slurm_scale_tailscale_authkey | quote }}"
          become: true
          when: "inventory_hostname.startswith('js2-gxit-')"

        - name: Copy slurm.conf
          copy:
            src: /etc/slurm/slurm.conf
            dest: /etc/slurm/slurm.conf
          become: true
          register: slurm_conf

        - name: Log IP address
          debug:
            var: ansible_host

        # This shouldn't be strictly necessary for started instances, but it can't hurt
        - name: Update slurm controller with instance IP
          command: scontrol update nodename={{ inventory_hostname }} nodeaddr={{ ansible_host }}
          delegate_to: localhost
          become: false

        - name: Start slurmd
          service:
            name: slurmd
            state: started
          become: true

        # FIXME: should not be doing this most likely
        #- name: Run slurmd -b to signal slurmctld
        #  command: /usr/sbin/slurmd -b
        #  become: true

      rescue:

        - name: Destroy existing instance on start failure
          os_server:
            cloud: "{{ cloud_id }}"
            name: "{{ inventory_hostname }}"
            state: absent
          delegate_to: localhost
          become: false

        - name: Fail due to previous failure
          fail:
            msg: failed
